
LLM이 어떤 빈칸 단어를 예측하는 것부터 시작함
LM은 랭귀지모델이고 대규모의 학습데이터를 LM모델에 학습시킨것을 LLM이라고 한다. 범용성이 높음

chatgpt , 바드 , 클로바 x , 라마2 등이 있음
chatgpt가 제일 성능이 좋음
라마2는 영어80퍼센트로 학습되있어서 한국어로 사용하기에는 무리가 있음

## 용어 정리
### 토큰화 
- 긴문장을 의미있는 단어로 자르는거 문장 , 단어 , 문자 단위로 자를 수있는데 문장과 단어사이 즉 의미있는 단어를 토큰화하는게 제일 좋음

### 인컨텍스트러닝(in-context learning)
  이 전에 bert를 이용하여 트레이닝 시켰다.(LLM이전)
	bert를 이용한 프리트레이닝이란 ?
- LM모델(기본모델)을 서브트레이닝하는([[파인튜닝]]) 방식으로 하여 감정분석잘하는 LM , 언어해석을 잘하는 LM등으로  트레이닝 시킬수있다.

대규모 LLM 자체가 이제는 많은것들을 해주니까 안에 있는 정보를 끌어내기위해서 프롬프트만으로도 충분한 정보를 가져올수있다고 이야기함 그게 -> 인컨텍스트러닝(프롬프트) -> 추가적인 데이터학습이 필요없어서 사용자 입장으로서 간편함

### 창발능력(Emergent Abilities)
- 파라미터 (학습데이터)가 어느정도 임계치가 넘어가면 창발능력 즉 기존 작은 LLM모델에서 발생하지 않았던 새로운 능력이 발현됨 
아예 못풀고있다가 파라미터가 많아짐에 따라 갑자기 풀수있는 능력이 생기는것을 말함 (예측 불가능한 현상)

### 온도
- LLM은 다음 단어가 어떤 단어가 들어갈지 후보가 쭉있고 확률도 쭉있음
- 온도가 작으면 (사실관계가 중요한 상황에서) 가장 그럴듯한 토큰이 뽑힐 확률이 높아짐 
- 온도가 크면 모든 토큰확률값이 평평해져 다양한 텍스트가 생성될 확률이 높아짐