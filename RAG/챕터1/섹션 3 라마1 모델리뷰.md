Liama1 모델 리뷰 -> 작은 파라미터로 좋은 성능을 줬음 -> 파라미터가 작더라도 토큰을 많이 생성해 최상의 성능을 달성함

GPU하나로도 돌릴수있음.

오픈소스임


표준 optimizer를 사용하여 대량의 텍스트 데이터에 대한 큰 트랜스포머를 훈련시켜야함 (트랜스포머 지식필요 )**

프리트레이닝은 우리가 못함 -> 돈 개많이 필요

### BPE
자연어처리를 위한 vocabulary집합구성방법

1. 문자 단위 구성 -> {'a':1,'b':2,'c':3,---}
2. 단어 단위 구성 -> {'apple':1,'banana':2} 단어 집합에 없는것은 unk로 표현함

데이터단위는 단어단위가 효율적임
근데 단어랑 문자단위 중간으로 하면 좋음
문자를 쭉펼치고 중복되는거 합쳐서 문자쌍으로 또 줄이고 줄여서 새로운 단위로 합쳐서 더이상 합칠 문자쌍이없거나 단어 집합 크기에 도달될떄 과정을반복하면 작게함

ex) aaabdaaabac -> ZabdZabac (Z=aa) -> ZYDZYac (Y=ab) -> XdXac (X=ZY)

좀더 효율적인 vocalbulary집합을 만듬